{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport re\nimport sys\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom nltk.stem import LancasterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"article_topics_df  = pd.read_csv(\"/kaggle/input/vmondaq-final/article_topics.csv\", engine='python')\narticles_df = pd.read_csv(\"/kaggle/input/vmondaq-final/articles.csv\", header=None)\ntopic_relationships_df = pd.read_csv(\"/kaggle/input/vmondaq-final/topic_relationships.csv\", engine='python')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"article_topics_df.shape, articles_df.shape, topic_relationships_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"article_topics_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df.columns = ['article_id', 'title', 'body','country_desc', 'category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"article_topics_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_relationships_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_relationships_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_relationships_df.columns = ['parent_topic_id', 'topic_id']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(article_topics_df, topic_relationships_df, on='topic_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['parent_topic_id'] = df['parent_topic_id'].fillna(df.topic_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df = articles_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_df.dtypes, df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = df[['article_id', 'parent_topic_id']]\nfinal_df['value'] = 1\nfinal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.pivot_table(final_df, values = ['value'], index=['article_id'], columns = 'parent_topic_id').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.fillna(0, inplace=True)\nresult_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_df = pd.DataFrame(result_df.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_df.columns = ['article_id' , 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7','topic_8',\n                     'topic_9', 'topic_10', 'topic_11', 'topic_12', 'topic_13', 'topic_14', 'topic_15', 'topic_16', 'topic_17',\n                     'topic_18', 'topic_19', 'topic_20', 'topic_21', 'topic_22', 'topic_23', 'topic_24', 'topic_25', 'topic_26',\n                     'topic_28', 'topic_29', 'topic_30', 'topic_31', 'topic_32']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_df = pd.merge(articles_df, f_df, on='article_id', how='left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = r_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['body']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Defining Function to Parse the HTML Text body of the articles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_text(html):\n    soup = BeautifulSoup(html)\n    text = soup.get_text()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom bs4 import BeautifulSoup\nimport string\nimport csv\ncsv.field_size_limit()\ncsv.field_size_limit(256<<10)\ncsv.field_size_limit()\ndata['clean_text'] = data['body'].apply(lambda x: parse_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lower casing\ndf['clean_text'] = df.clean_text.map(\n    lambda x: x.lower().translate(str.maketrans('','', string.punctuation))\n)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Lower Casing\n\nLower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n\nThis is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\nBy default, lower casing is done my most of the modern day vecotirzers and tokenizers like sklearn TfidfVectorizer and Keras Tokenizer. So we need to set them to false as needed depending on our use case.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text_lower\"] = df[\"clean_text\"].str.lower()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Removal of Punctuations\n\nOne another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n\nWe also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n\n!\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`\n\nWe can add or remove more punctuations as per our need.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text_wo_punct\"] = df[\"text_lower\"].apply(lambda text: remove_punctuation(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Removal of stopwords\n\nStopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n\nThese stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ncnt = Counter()\nfor text in df[\"text_wo_stop\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Removal of Frequent words\n\nIn the previous preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\n\nSo this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.\n\nLet us get the most common words and then remove them in the next step","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndf[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Removal of Rare words\n\nThis is very similar to previous preprocessing step but we will remove the rare words from the corpus.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n\ndf[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Stemming\n\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From Wikipedia)\n\nFor example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. But say in another example, we have two words console and consoling, the stemmer will remove the suffix and make them consol which is not a proper english word.\n\nThere are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer \n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndf[\"text_stemmed\"] = df[\"text_wo_stopfreqrare\"].apply(lambda text: stem_words(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Lemmatization\n\nLemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n\nAs a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\n\nLet us use the WordNetLemmatizer in nltk to lemmatize our sentences\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf[\"text_lemmatized\"] = df[\"text_wo_stopfreqrare\"].apply(lambda text: lemmatize_words(text))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df['clean_text']\ny= df.drop(['article_id', 'title', 'body', 'country_desc', 'category', 'clean_text', 'text_lower', 'text_wo_punct', 'text_wo_stop',\n       'text_wo_stopfreq', 'text_wo_stopfreqrare', 'text_stemmed',\n       'text_lemmatized' ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=144)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words = set(stopwords.words('english')))\nvectorizer.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_tfidf_vec = vectorizer.transform(X_train)\nx_test_tfidf_vec = vectorizer.transform(X_test)\nprint(x_train_tfidf_vec.shape, x_test_tfidf_vec.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_tfidf_vec.shape, x_test_tfidf_vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelling(clf, x_train, y_train, x_test, y_test):\n    \n  \n    \n    clf.fit(x_train, y_train)\n    y_train_pred = clf.predict(x_train)\n    \n    y_test_pred = clf.predict(x_test)\n    \n    vals = precision_recall_fscore_support(y_test, y_test_pred, average='macro')\n    precision = vals[0]\n    recall = vals[1]\n    f1 = vals[2]\n    acc = accuracy_score(y_test, y_test_pred)\n    print(\"accuracy: \", acc, f1)\n    print(\"confusion matrix for CV is \")\n    print(confusion_matrix(y_test, y_test_pred ))\n    \n    return y_train_pred, y_test_pred, precision, recall, f1, acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred, y_test_pred, precision, recall, f1, acc = modelling(LogisticRegression(C = 1), \n                                                                            x_train_tfidf_vec, \n                                                                            y_train['topic_2'], \n                                                                            x_test_tfidf_vec,\n                                                                             y_test['topic_2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}